---
title: Cognitive Planning using LLMs for ROS 2
description: Comprehensive guide to using Large Language Models for cognitive planning in humanoid robotics with ROS 2 integration
sidebar_label: Cognitive Planning with LLMs
---

# Cognitive Planning using LLMs for ROS 2

## Introduction to AI-Based Cognitive Planning

Cognitive planning represents the intelligence layer in humanoid robotics that transforms simple voice commands into complex, executable action sequences. This critical component bridges natural language understanding with robot action execution, enabling robots to interpret high-level commands and break them down into specific, coordinated behaviors.

Large Language Models (LLMs) excel at this cognitive planning task because they can understand the nuances of natural language, infer implicit requirements, and generate structured action plans that account for the robot's capabilities and environmental constraints. For humanoid robots operating in human environments, this cognitive layer is essential for creating intuitive and flexible human-robot interaction.

### The Cognitive Planning Pipeline

The cognitive planning process involves several key stages:

1. **Natural Language Understanding**: Interpreting the user's intent from the voice command
2. **Context Awareness**: Understanding the current environment and robot state
3. **Action Decomposition**: Breaking complex commands into executable steps
4. **Constraint Checking**: Ensuring actions are feasible given robot capabilities
5. **Plan Generation**: Creating a sequence of ROS 2 actions
6. **Validation and Refinement**: Ensuring the plan is safe and achievable

### Why LLMs for Cognitive Planning?

LLMs offer several advantages for cognitive planning in robotics:

- **Natural Language Proficiency**: LLMs excel at understanding the subtleties and context of natural language commands
- **World Knowledge**: Pre-trained models contain vast amounts of knowledge about objects, actions, and their relationships
- **Reasoning Capabilities**: LLMs can perform multi-step reasoning to decompose complex tasks
- **Flexibility**: Can handle novel commands and adapt to different contexts
- **Contextual Understanding**: Can maintain context across multiple interactions

## LLM Integration for Natural Language Understanding

### Selecting the Right LLM

For cognitive planning in humanoid robotics, the choice of LLM is crucial. Considerations include:

- **Response Quality**: Accuracy in understanding commands and generating valid action sequences
- **Latency**: Processing speed for real-time interaction
- **Cost**: Operational costs for API-based models
- **Reliability**: Consistency and availability of the service
- **Safety**: Built-in safety mechanisms to prevent unsafe action generation

### OpenAI GPT Integration

OpenAI's GPT models are excellent choices for cognitive planning due to their strong reasoning capabilities and structured output potential:

```python
import openai
import json
import os
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class ActionPlan:
    """
    Represents a sequence of robot actions generated by the AI system
    """
    id: str
    command_id: str
    actions: List[Dict[str, Any]]
    created_at: str
    estimated_duration: float
    complexity_score: int
    status: str = "PLANNED"
    execution_log: List[Dict[str, str]] = None

@dataclass
class RobotAction:
    """
    An individual action that can be executed by the robot
    """
    id: str
    action_type: str
    parameters: Dict[str, Any]
    priority: int
    timeout: float
    preconditions: List[str]
    postconditions: List[str]
    dependencies: List[str]

class LLMCognitivePlanner:
    """
    Cognitive planner using LLMs for natural language understanding and action planning
    """
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.model = "gpt-4-turbo"  # or gpt-3.5-turbo for cost/performance trade-offs

    def plan_actions_from_command(self, command: str, robot_state: Dict[str, Any] = None,
                                  user_context: Dict[str, Any] = None) -> ActionPlan:
        """
        Plan robot actions based on a natural language command
        """
        system_prompt = self._build_system_prompt(robot_state)
        user_prompt = self._build_user_prompt(command, user_context)

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,  # Low temperature for more consistent, structured output
            functions=[
                {
                    "name": "generate_action_plan",
                    "description": "Generate a structured action plan for the robot",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "actions": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "action_type": {"type": "string", "enum": ["NAVIGATION", "MANIPULATION", "INTERACTION", "SENSING", "OTHER"]},
                                        "parameters": {"type": "object"},
                                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                                        "timeout": {"type": "number", "minimum": 0},
                                        "preconditions": {"type": "array", "items": {"type": "string"}},
                                        "postconditions": {"type": "array", "items": {"type": "string"}},
                                        "dependencies": {"type": "array", "items": {"type": "string"}}
                                    },
                                    "required": ["action_type", "parameters"]
                                }
                            },
                            "estimated_duration": {"type": "number", "minimum": 0},
                            "complexity_score": {"type": "integer", "minimum": 1, "maximum": 10}
                        },
                        "required": ["actions", "estimated_duration", "complexity_score"]
                    }
                }
            ],
            function_call={"name": "generate_action_plan"}
        )

        # Extract the function arguments
        function_args = json.loads(response.choices[0].message.function_call.arguments)

        # Create RobotAction objects from the response
        robot_actions = []
        for i, action_data in enumerate(function_args["actions"]):
            robot_action = RobotAction(
                id=f"action_{i}",
                action_type=action_data["action_type"],
                parameters=action_data.get("parameters", {}),
                priority=action_data.get("priority", 3),
                timeout=action_data.get("timeout", 30.0),
                preconditions=action_data.get("preconditions", []),
                postconditions=action_data.get("postconditions", []),
                dependencies=action_data.get("dependencies", [])
            )
            robot_actions.append(robot_action)

        # Create and return the ActionPlan
        action_plan = ActionPlan(
            id=f"plan_{int(openai.util.current_timestamp())}",
            command_id=f"cmd_{int(openai.util.current_timestamp())}",  # In real implementation, this would come from voice recognition
            actions=robot_actions,
            created_at=str(openai.util.current_timestamp()),
            estimated_duration=function_args["estimated_duration"],
            complexity_score=function_args["complexity_score"]
        )

        return action_plan

    def _build_system_prompt(self, robot_state: Dict[str, Any]) -> str:
        """
        Build the system prompt with robot capabilities and constraints
        """
        capabilities = [
            "NAVIGATION: move to locations, follow paths",
            "MANIPULATION: pick up objects, place objects, open/close",
            "INTERACTION: speak, gesture, respond to humans",
            "SENSING: detect objects, recognize faces, measure distances"
        ]

        constraints = [
            "Robot cannot go through walls or obstacles",
            "Robot cannot manipulate objects beyond its reach",
            "Robot must maintain balance during actions",
            "Robot should avoid unsafe situations"
        ]

        system_prompt = f"""
        You are a cognitive planning system for a humanoid robot. Your role is to interpret natural language commands
        and generate structured action plans that the robot can execute.

        Robot Capabilities:
        {chr(10).join(capabilities)}

        Robot Constraints:
        {chr(10).join(constraints)}

        Current Robot State:
        {json.dumps(robot_state, indent=2) if robot_state else 'Unknown'}

        Generate action plans that:
        1. Are feasible given the robot's capabilities
        2. Respect the robot's physical constraints
        3. Account for the current environment and state
        4. Include appropriate preconditions and postconditions
        5. Sequence actions logically with proper dependencies

        Return the plan in the requested structured format.
        """

        return system_prompt

    def _build_user_prompt(self, command: str, user_context: Dict[str, Any]) -> str:
        """
        Build the user prompt with the command and context
        """
        context_str = f"User Context: {json.dumps(user_context, indent=2)}" if user_context else "No user context provided"

        user_prompt = f"""
        {context_str}

        Command: "{command}"

        Please generate a detailed action plan for the robot to execute this command.
        Break down complex commands into simple, executable actions.
        Consider the robot's current state and environment when planning.
        """

        return user_prompt
```

### Alternative LLM Integration Patterns

For different deployment scenarios, various LLM integration patterns can be used:

#### 1. Function Calling Approach

The example above demonstrates function calling, which is excellent for structured output:

```python
def plan_with_function_calling(self, command: str):
    """
    Use OpenAI's function calling for structured output
    """
    # Implementation shown in the previous example
    pass
```

#### 2. JSON Mode Approach

For models that support JSON mode, this ensures valid JSON output:

```python
def plan_with_json_mode(self, command: str):
    """
    Use JSON mode for guaranteed structured output
    """
    system_prompt = self._build_system_prompt()
    user_prompt = self._build_user_prompt(command)

    response = openai.ChatCompletion.create(
        model=self.model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        response_format={"type": "json_object"},  # Ensures valid JSON output
        temperature=0.1
    )

    # Parse the JSON response
    result = json.loads(response.choices[0].message.content)
    return self._parse_action_plan(result)
```

#### 3. Self-Hosted Model Integration

For privacy or latency requirements, self-hosted models can be used:

```python
import requests
from transformers import pipeline

class SelfHostedCognitivePlanner:
    """
    Cognitive planner using self-hosted models
    """
    def __init__(self, model_endpoint: str = None):
        if model_endpoint:
            self.model_endpoint = model_endpoint
            self.use_api = True
        else:
            # Use local model
            self.generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
            self.use_api = False

    def plan_actions_locally(self, command: str):
        """
        Plan actions using a local model
        """
        if self.use_api:
            # Call self-hosted model API
            response = requests.post(
                self.model_endpoint,
                json={"prompt": command, "max_tokens": 500}
            )
            return response.json()
        else:
            # Use local model (simplified example)
            prompt = f"Convert this command to robot actions: {command}. Respond with a JSON list of actions."
            result = self.generator(prompt, max_length=200, num_return_sequences=1)
            return result[0]['generated_text']
```

## Command Interpretation and Action Mapping

### Understanding Command Semantics

Effective command interpretation requires understanding not just the literal words but the underlying intent and context. This involves several layers of processing:

1. **Syntactic Analysis**: Understanding the grammatical structure
2. **Semantic Analysis**: Extracting meaning from the command
3. **Contextual Analysis**: Understanding the command in environmental context
4. **Intent Classification**: Determining the high-level goal
5. **Action Decomposition**: Breaking down into executable steps

### Command Classification System

```python
from enum import Enum
from typing import Tuple

class CommandCategory(Enum):
    NAVIGATION = "navigation"
    MANIPULATION = "manipulation"
    INTERACTION = "interaction"
    INFORMATION = "information"
    COMPLEX = "complex"

class CommandClassifier:
    """
    Classifies commands to guide the planning process
    """
    def __init__(self):
        self.navigation_keywords = [
            "go to", "move to", "walk to", "navigate to", "follow",
            "move forward", "move backward", "turn left", "turn right",
            "go", "move", "walk", "run", "step", "approach"
        ]

        self.manipulation_keywords = [
            "pick up", "grab", "take", "get", "bring", "place",
            "put", "set", "hold", "release", "open", "close",
            "lift", "drop", "catch", "throw", "move"
        ]

        self.interaction_keywords = [
            "say", "speak", "tell", "greet", "hello", "hi",
            "introduce", "wave", "dance", "pose", "gesture",
            "look at", "face", "turn to", "attend to"
        ]

    def classify_command(self, command: str) -> Tuple[CommandCategory, float]:
        """
        Classify a command and return the category with confidence
        """
        command_lower = command.lower()

        # Count keywords for each category
        nav_count = sum(1 for keyword in self.navigation_keywords if keyword in command_lower)
        manip_count = sum(1 for keyword in self.manipulation_keywords if keyword in command_lower)
        inter_count = sum(1 for keyword in self.interaction_keywords if keyword in command_lower)

        # Determine the dominant category
        max_count = max(nav_count, manip_count, inter_count)

        if max_count == 0:
            # If no clear keywords, default to complex or information
            return CommandCategory.COMPLEX, 0.5

        if nav_count == max_count:
            category = CommandCategory.NAVIGATION
        elif manip_count == max_count:
            category = CommandCategory.MANIPULATION
        else:
            category = CommandCategory.INTERACTION

        # Calculate confidence based on keyword density
        total_keywords = nav_count + manip_count + inter_count
        confidence = min(total_keywords * 0.3, 1.0)  # Max 1.0 confidence

        return category, confidence

# Integration with the cognitive planner
class EnhancedLLMCognitivePlanner(LLMCognitivePlanner):
    def __init__(self, api_key: str):
        super().__init__(api_key)
        self.classifier = CommandClassifier()

    def plan_actions_from_command(self, command: str, robot_state: Dict[str, Any] = None,
                                  user_context: Dict[str, Any] = None) -> ActionPlan:
        """
        Enhanced planning with command classification
        """
        # First, classify the command
        category, confidence = self.classifier.classify_command(command)

        # Include classification in the system prompt for better planning
        system_prompt = self._build_enhanced_system_prompt(robot_state, category, confidence)

        # Continue with normal planning process
        user_prompt = self._build_user_prompt(command, user_context)

        # Add classification guidance to the prompt
        enhanced_user_prompt = f"""
        Command Category: {category.value} (Confidence: {confidence:.2f})

        {user_prompt}

        Given the command category, focus your planning on the appropriate types of actions.
        For {category.value} commands, prioritize the relevant capabilities.
        """

        # Rest of the implementation remains the same
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_user_prompt}
            ],
            temperature=0.1,
            functions=[
                {
                    "name": "generate_action_plan",
                    "description": "Generate a structured action plan for the robot",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "actions": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "action_type": {"type": "string", "enum": ["NAVIGATION", "MANIPULATION", "INTERACTION", "SENSING", "OTHER"]},
                                        "parameters": {"type": "object"},
                                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                                        "timeout": {"type": "number", "minimum": 0},
                                        "preconditions": {"type": "array", "items": {"type": "string"}},
                                        "postconditions": {"type": "array", "items": {"type": "string"}},
                                        "dependencies": {"type": "array", "items": {"type": "string"}}
                                    },
                                    "required": ["action_type", "parameters"]
                                }
                            },
                            "estimated_duration": {"type": "number", "minimum": 0},
                            "complexity_score": {"type": "integer", "minimum": 1, "maximum": 10}
                        },
                        "required": ["actions", "estimated_duration", "complexity_score"]
                    }
                }
            ],
            function_call={"name": "generate_action_plan"}
        )

        # Process the response as before
        function_args = json.loads(response.choices[0].message.function_call.arguments)

        robot_actions = []
        for i, action_data in enumerate(function_args["actions"]):
            robot_action = RobotAction(
                id=f"action_{i}",
                action_type=action_data["action_type"],
                parameters=action_data.get("parameters", {}),
                priority=action_data.get("priority", 3),
                timeout=action_data.get("timeout", 30.0),
                preconditions=action_data.get("preconditions", []),
                postconditions=action_data.get("postconditions", []),
                dependencies=action_data.get("dependencies", [])
            )
            robot_actions.append(robot_action)

        action_plan = ActionPlan(
            id=f"plan_{int(openai.util.current_timestamp())}",
            command_id=f"cmd_{int(openai.util.current_timestamp())}",
            actions=robot_actions,
            created_at=str(openai.util.current_timestamp()),
            estimated_duration=function_args["estimated_duration"],
            complexity_score=function_args["complexity_score"]
        )

        return action_plan

    def _build_enhanced_system_prompt(self, robot_state: Dict[str, Any],
                                    category: CommandCategory, confidence: float) -> str:
        """
        Build system prompt with command classification information
        """
        base_prompt = self._build_system_prompt(robot_state)

        category_guidance = f"""
        The command has been classified as {category.value} with confidence {confidence:.2f}.
        This suggests the primary focus should be on {category.value}-related actions.
        However, complex tasks may require multiple action types.
        """

        return f"{base_prompt}\n\n{category_guidance}"
```

### Semantic Parsing and Entity Recognition

For more sophisticated command interpretation, semantic parsing can extract specific entities and relationships:

```python
import re
from typing import NamedTuple

class ParsedEntity(NamedTuple):
    entity_type: str
    value: str
    confidence: float

class SemanticParser:
    """
    Performs semantic parsing to extract entities from commands
    """
    def __init__(self):
        # Define patterns for common entities in robot commands
        self.patterns = {
            "location": [
                r"to the (\w+)",           # "go to the kitchen"
                r"in the (\w+)",           # "find object in the room"
                r"at the (\w+)",           # "wait at the door"
            ],
            "object": [
                r"(?:a|an|the) (\w+)",     # "pick up a cup"
                r"(\w+) on the",           # "the book on the table"
                r"(\w+) in the",           # "the keys in the drawer"
            ],
            "person": [
                r"to (\w+)",               # "bring this to John"
                r"(\w+)'s",                # "bring John's book"
            ],
            "measurement": [
                r"(\d+(?:\.\d+)?) (meters|cm|feet|inches)",  # "move 2 meters forward"
                r"(\d+(?:\.\d+)?) (seconds|minutes)",        # "wait 5 seconds"
            ],
            "direction": [
                r"(forward|backward|left|right|up|down)",    # "move forward"
                r"(north|south|east|west)",                  # "face north"
            ]
        }

    def parse_command(self, command: str) -> List[ParsedEntity]:
        """
        Parse a command to extract entities
        """
        entities = []

        for entity_type, patterns in self.patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, command, re.IGNORECASE)
                for match in matches:
                    entity_value = match.group(1)  # Get the first captured group
                    entities.append(ParsedEntity(
                        entity_type=entity_type,
                        value=entity_value,
                        confidence=0.8  # Base confidence for regex matches
                    ))

        return entities

# Integration example
class SemanticAwarePlanner(EnhancedLLMCognitivePlanner):
    def __init__(self, api_key: str):
        super().__init__(api_key)
        self.semantic_parser = SemanticParser()

    def plan_with_semantic_context(self, command: str, robot_state: Dict[str, Any] = None):
        """
        Plan actions using semantic context from the command
        """
        # Parse the command for entities
        entities = self.semantic_parser.parse_command(command)

        # Create a semantic context description
        semantic_context = self._format_entities_for_prompt(entities)

        # Build enhanced prompt with semantic information
        system_prompt = self._build_semantic_aware_system_prompt(robot_state, semantic_context)
        user_prompt = f"Command: '{command}'\n\nSemantic Context: {semantic_context}"

        # Continue with normal planning process using enhanced context
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            functions=[
                {
                    "name": "generate_action_plan",
                    "description": "Generate a structured action plan for the robot",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "actions": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "action_type": {"type": "string", "enum": ["NAVIGATION", "MANIPULATION", "INTERACTION", "SENSING", "OTHER"]},
                                        "parameters": {"type": "object"},
                                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                                        "timeout": {"type": "number", "minimum": 0},
                                        "preconditions": {"type": "array", "items": {"type": "string"}},
                                        "postconditions": {"type": "array", "items": {"type": "string"}},
                                        "dependencies": {"type": "array", "items": {"type": "string"}}
                                    },
                                    "required": ["action_type", "parameters"]
                                }
                            },
                            "estimated_duration": {"type": "number", "minimum": 0},
                            "complexity_score": {"type": "integer", "minimum": 1, "maximum": 10}
                        },
                        "required": ["actions", "estimated_duration", "complexity_score"]
                    }
                }
            ],
            function_call={"name": "generate_action_plan"}
        )

        # Process response as before...
        function_args = json.loads(response.choices[0].message.function_call.arguments)

        robot_actions = []
        for i, action_data in enumerate(function_args["actions"]):
            robot_action = RobotAction(
                id=f"action_{i}",
                action_type=action_data["action_type"],
                parameters=action_data.get("parameters", {}),
                priority=action_data.get("priority", 3),
                timeout=action_data.get("timeout", 30.0),
                preconditions=action_data.get("preconditions", []),
                postconditions=action_data.get("postconditions", []),
                dependencies=action_data.get("dependencies", [])
            )
            robot_actions.append(robot_action)

        action_plan = ActionPlan(
            id=f"plan_{int(openai.util.current_timestamp())}",
            command_id=f"cmd_{int(openai.util.current_timestamp())}",
            actions=robot_actions,
            created_at=str(openai.util.current_timestamp()),
            estimated_duration=function_args["estimated_duration"],
            complexity_score=function_args["complexity_score"]
        )

        return action_plan

    def _format_entities_for_prompt(self, entities: List[ParsedEntity]) -> str:
        """
        Format entities for inclusion in the LLM prompt
        """
        if not entities:
            return "No specific entities identified in the command."

        formatted = []
        for entity in entities:
            formatted.append(f"{entity.entity_type.upper()}: {entity.value} (confidence: {entity.confidence:.2f})")

        return "; ".join(formatted)

    def _build_semantic_aware_system_prompt(self, robot_state: Dict[str, Any], semantic_context: str) -> str:
        """
        Build system prompt that incorporates semantic context
        """
        base_prompt = self._build_system_prompt(robot_state)

        semantic_guidance = f"""
        Semantic Analysis of Command:
        {semantic_context}

        Use this semantic information to guide your action planning:
        - Locations: Plan navigation to these locations
        - Objects: Identify and manipulate these objects
        - People: Interact with these individuals
        - Measurements: Use these quantities for precise actions
        - Directions: Move or orient in these directions
        """

        return f"{base_prompt}\n\n{semantic_guidance}"
```

## Action Planning and Sequence Generation

### Planning Algorithm Overview

The action planning process transforms high-level commands into executable action sequences. This involves:

1. **Decomposition**: Breaking complex commands into simpler subtasks
2. **Sequencing**: Ordering actions logically with dependencies
3. **Constraint Checking**: Ensuring actions are feasible
4. **Optimization**: Finding efficient execution sequences
5. **Validation**: Ensuring the plan achieves the goal

### Hierarchical Task Network (HTN) Planning

For complex robotic tasks, HTN planning can be very effective:

```python
from typing import Protocol, List
from abc import ABC, abstractmethod

class Task(ABC):
    """Abstract base class for tasks in the planning system"""

    @abstractmethod
    def can_execute(self, state: Dict[str, Any]) -> bool:
        """Check if this task can be executed given the current state"""
        pass

    @abstractmethod
    def get_subtasks(self, state: Dict[str, Any]) -> List['Task']:
        """Get subtasks needed to achieve this task"""
        pass

    @abstractmethod
    def to_robot_action(self) -> RobotAction:
        """Convert this task to a robot action"""
        pass

class NavigationTask(Task):
    """Task for navigating to a location"""

    def __init__(self, target_location: str):
        self.target_location = target_location

    def can_execute(self, state: Dict[str, Any]) -> bool:
        # Check if navigation is possible (e.g., path is clear, robot is operational)
        return state.get("robot_operational", True)

    def get_subtasks(self, state: Dict[str, Any]) -> List[Task]:
        # Navigation might involve subtasks like path planning, obstacle avoidance
        return []

    def to_robot_action(self) -> RobotAction:
        return RobotAction(
            id=f"nav_to_{self.target_location}",
            action_type="NAVIGATION",
            parameters={"target_location": self.target_location},
            priority=3,
            timeout=120.0,
            preconditions=["robot_operational", f"path_to_{self.target_location}_clear"],
            postconditions=[f"at_{self.target_location}"],
            dependencies=[]
        )

class ManipulationTask(Task):
    """Task for manipulating an object"""

    def __init__(self, action: str, object_name: str):
        self.action = action  # e.g., "pick_up", "place_down"
        self.object_name = object_name

    def can_execute(self, state: Dict[str, Any]) -> bool:
        # Check if manipulation is possible (e.g., object is reachable, gripper is free)
        return (state.get("gripper_free", True) and
                state.get("object_reachable", {}).get(self.object_name, False))

    def get_subtasks(self, state: Dict[str, Any]) -> List[Task]:
        # Manipulation might require approaching the object first
        if self.action in ["pick_up", "grasp"]:
            return [NavigationTask(f"near_{self.object_name}")]
        return []

    def to_robot_action(self) -> RobotAction:
        return RobotAction(
            id=f"{self.action}_{self.object_name}",
            action_type="MANIPULATION",
            parameters={"action": self.action, "object": self.object_name},
            priority=4,
            timeout=60.0,
            preconditions=["gripper_free", f"{self.object_name}_reachable"],
            postconditions=[f"object_{self.object_name}_{self.action}"],
            dependencies=[]
        )

class HTNCognitivePlanner:
    """
    Cognitive planner using Hierarchical Task Networks
    """
    def __init__(self, api_key: str):
        self.llm_planner = SemanticAwarePlanner(api_key)

    def plan_with_htn(self, command: str, robot_state: Dict[str, Any]) -> ActionPlan:
        """
        Plan actions using HTN approach combined with LLM guidance
        """
        # First, use LLM to understand the high-level plan structure
        high_level_plan = self.llm_planner.plan_with_semantic_context(command, robot_state)

        # Convert to HTN tasks and refine
        htn_tasks = self._convert_to_htn_tasks(high_level_plan.actions)

        # Perform HTN-style refinement
        refined_tasks = self._refine_tasks(htn_tasks, robot_state)

        # Convert back to action plan
        refined_actions = [task.to_robot_action() for task in refined_tasks]

        # Create new action plan with refined actions
        refined_plan = ActionPlan(
            id=f"refined_{high_level_plan.id}",
            command_id=high_level_plan.command_id,
            actions=refined_actions,
            created_at=high_level_plan.created_at,
            estimated_duration=self._estimate_duration(refined_actions),
            complexity_score=high_level_plan.complexity_score
        )

        return refined_plan

    def _convert_to_htn_tasks(self, actions: List[RobotAction]) -> List[Task]:
        """
        Convert RobotAction objects to HTN Task objects for refinement
        """
        tasks = []
        for action in actions:
            if action.action_type == "NAVIGATION":
                target = action.parameters.get("target_location", "unknown")
                tasks.append(NavigationTask(target))
            elif action.action_type in ["MANIPULATION", "GRASP", "PLACE"]:
                obj_name = action.parameters.get("object", "unknown")
                action_name = action.parameters.get("action", "manipulate")
                tasks.append(ManipulationTask(action_name, obj_name))
            else:
                # For other action types, create a generic task
                tasks.append(GenericTask(action))

        return tasks

    def _refine_tasks(self, tasks: List[Task], state: Dict[str, Any]) -> List[Task]:
        """
        Refine tasks by expanding high-level tasks into detailed subtasks
        """
        refined = []
        for task in tasks:
            if task.can_execute(state):
                # Add any required subtasks first
                subtasks = task.get_subtasks(state)
                refined.extend(subtasks)
                # Then add the main task
                refined.append(task)
            else:
                # If the task can't be executed, we need to plan how to make it executable
                refined.extend(self._make_task_executable(task, state))

        return refined

    def _make_task_executable(self, task: Task, state: Dict[str, Any]) -> List[Task]:
        """
        Plan actions to make a task executable
        """
        # This is a simplified implementation
        # In practice, this would involve more sophisticated planning
        return [task]  # For now, just return the task itself

    def _estimate_duration(self, actions: List[RobotAction]) -> float:
        """
        Estimate total duration of action sequence
        """
        return sum(action.timeout for action in actions) * 0.7  # Assuming 70% efficiency

class GenericTask(Task):
    """Generic task wrapper for RobotAction objects"""

    def __init__(self, robot_action: RobotAction):
        self.robot_action = robot_action

    def can_execute(self, state: Dict[str, Any]) -> bool:
        # Check preconditions
        for condition in self.robot_action.preconditions:
            if not state.get(condition, False):
                return False
        return True

    def get_subtasks(self, state: Dict[str, Any]) -> List[Task]:
        # For generic tasks, return empty list (no subtasks)
        return []

    def to_robot_action(self) -> RobotAction:
        return self.robot_action
```

### Multi-Step Command Processing

For complex commands that require multiple steps, the planner needs to handle dependencies and sequencing:

```python
class MultiStepCommandProcessor:
    """
    Handles complex multi-step commands with dependencies
    """
    def __init__(self, cognitive_planner: HTNCognitivePlanner):
        self.planner = cognitive_planner

    def process_complex_command(self, command: str, robot_state: Dict[str, Any]) -> ActionPlan:
        """
        Process a complex command that might require multiple planning iterations
        """
        # First, get initial plan from LLM
        initial_plan = self.planner.plan_with_htn(command, robot_state)

        # Validate and refine the plan
        refined_plan = self._validate_and_refine_plan(initial_plan, robot_state)

        # Optimize the sequence
        optimized_plan = self._optimize_action_sequence(refined_plan, robot_state)

        return optimized_plan

    def _validate_and_refine_plan(self, plan: ActionPlan, state: Dict[str, Any]) -> ActionPlan:
        """
        Validate the plan and refine if necessary
        """
        # Check for conflicts in the plan
        conflicts = self._find_conflicts(plan.actions)

        if conflicts:
            # Resolve conflicts by replanning affected parts
            resolved_actions = self._resolve_conflicts(plan.actions, conflicts, state)
            plan.actions = resolved_actions

        # Check for missing preconditions
        missing_preconditions = self._check_missing_preconditions(plan.actions, state)

        if missing_preconditions:
            # Add actions to satisfy missing preconditions
            plan.actions = self._add_precondition_actions(plan.actions, missing_preconditions, state)

        return plan

    def _find_conflicts(self, actions: List[RobotAction]) -> List[Dict[str, Any]]:
        """
        Find conflicts between actions (e.g., resource conflicts, contradictory goals)
        """
        conflicts = []

        # Check for resource conflicts (e.g., gripper used by multiple actions)
        gripper_actions = [i for i, action in enumerate(actions)
                          if action.action_type == "MANIPULATION" and
                             action.parameters.get("action") in ["pick_up", "grasp"]]

        if len(gripper_actions) > 1:
            conflicts.append({
                "type": "resource_conflict",
                "resource": "gripper",
                "actions": gripper_actions,
                "description": "Multiple actions require gripper simultaneously"
            })

        # Check for contradictory postconditions
        for i, action1 in enumerate(actions):
            for j, action2 in enumerate(actions[i+1:], i+1):
                # Check if action1's postcondition contradicts action2's postcondition
                for post1 in action1.postconditions:
                    for post2 in action2.postconditions:
                        if (post1.startswith("not_") and post2 == post1[4:]) or \
                           (post2.startswith("not_") and post1 == post2[4:]):
                            conflicts.append({
                                "type": "contradiction",
                                "actions": [i, j],
                                "description": f"Contradictory postconditions: {post1} vs {post2}"
                            })

        return conflicts

    def _resolve_conflicts(self, actions: List[RobotAction], conflicts: List[Dict], state: Dict[str, Any]) -> List[RobotAction]:
        """
        Resolve conflicts in the action plan
        """
        resolved_actions = actions.copy()

        for conflict in conflicts:
            if conflict["type"] == "resource_conflict":
                # For resource conflicts, add dependencies to serialize access
                resource_actions = [resolved_actions[i] for i in conflict["actions"]]

                # Add dependencies so resource is used sequentially
                for i in range(1, len(resource_actions)):
                    prev_action_idx = conflict["actions"][i-1]
                    curr_action_idx = conflict["actions"][i]

                    # Add dependency: current action depends on previous action completing
                    resolved_actions[curr_action_idx].dependencies.append(
                        resolved_actions[prev_action_idx].id
                    )

        return resolved_actions

    def _check_missing_preconditions(self, actions: List[RobotAction], state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Check for missing preconditions that aren't satisfied by the plan
        """
        missing = []

        # Track which conditions are established by the plan
        established_conditions = set(state.keys())  # Start with initial state

        for i, action in enumerate(actions):
            for precondition in action.preconditions:
                if precondition not in established_conditions:
                    missing.append({
                        "action_index": i,
                        "precondition": precondition,
                        "action_id": action.id
                    })

            # Add this action's postconditions to established conditions
            established_conditions.update(action.postconditions)

        return missing

    def _add_precondition_actions(self, actions: List[RobotAction], missing: List[Dict], state: Dict[str, Any]) -> List[RobotAction]:
        """
        Add actions to satisfy missing preconditions
        """
        updated_actions = actions.copy()

        for missing_item in missing:
            action_idx = missing_item["action_index"]
            precondition = missing_item["precondition"]

            # Create an action to satisfy the missing precondition
            # This is a simplified approach - in practice, you'd have more sophisticated methods
            if precondition == "gripper_free":
                # Add an action to release gripper if it's currently holding something
                release_action = RobotAction(
                    id=f"release_gripper_before_{actions[action_idx].id}",
                    action_type="MANIPULATION",
                    parameters={"action": "release"},
                    priority=5,
                    timeout=10.0,
                    preconditions=[],
                    postconditions=["gripper_free"],
                    dependencies=[]
                )

                # Insert before the action that needs the precondition
                updated_actions.insert(action_idx, release_action)

        return updated_actions

    def _optimize_action_sequence(self, plan: ActionPlan, state: Dict[str, Any]) -> ActionPlan:
        """
        Optimize the action sequence for efficiency
        """
        # Simple optimization: combine similar actions when possible
        optimized_actions = self._combine_similar_actions(plan.actions)

        # Update the plan with optimized actions
        plan.actions = optimized_actions
        plan.estimated_duration = self._estimate_duration(optimized_actions)

        return plan

    def _combine_similar_actions(self, actions: List[RobotAction]) -> List[RobotAction]:
        """
        Combine similar actions to improve efficiency
        """
        if not actions:
            return actions

        combined = [actions[0]]  # Start with first action

        for current_action in actions[1:]:
            last_action = combined[-1]

            # Check if we can combine these actions
            if (current_action.action_type == last_action.action_type and
                current_action.action_type == "NAVIGATION"):
                # For navigation, we might combine if they're part of a path
                # This is a simplified check
                pass
            else:
                # If they can't be combined, add as separate action
                combined.append(current_action)

        return combined

    def _estimate_duration(self, actions: List[RobotAction]) -> float:
        """
        Estimate total duration of action sequence
        """
        return sum(action.timeout for action in actions) * 0.7  # Assuming 70% efficiency
```

## ROS 2 Action Message Conversion

### ROS 2 Action Architecture

ROS 2 provides a robust action architecture that's ideal for humanoid robot tasks. Actions are appropriate for long-running tasks that provide feedback during execution. The cognitive planner needs to convert high-level plans into ROS 2 action messages that the robot can execute.

### Action Message Definitions

For humanoid robotics, common action types include:

```python
# These would typically be defined in ROS 2 package action files (.action)
# Example action definition for navigation (Nav2):
"""
# Goal: Target pose for navigation
geometry_msgs/PoseStamped pose
string behavior_tree

---
# Result: Final outcome of navigation
int32 code
string message

---
# Feedback: Current progress of navigation
geometry_msgs/PoseStamped current_pose
string message
float32 distance_remaining
"""
```

### Action Conversion System

```python
from typing import Any, Dict
import json

class ROS2ActionConverter:
    """
    Converts planned actions to ROS 2 action messages
    """
    def __init__(self):
        self.action_type_mapping = {
            "NAVIGATION": self._convert_navigation_action,
            "MANIPULATION": self._convert_manipulation_action,
            "INTERACTION": self._convert_interaction_action,
            "SENSING": self._convert_sensing_action
        }

    def convert_to_ros2_action(self, robot_action: RobotAction) -> Dict[str, Any]:
        """
        Convert a RobotAction to a ROS 2 action message structure
        """
        converter = self.action_type_mapping.get(robot_action.action_type)
        if not converter:
            raise ValueError(f"Unknown action type: {robot_action.action_type}")

        return converter(robot_action)

    def _convert_navigation_action(self, action: RobotAction) -> Dict[str, Any]:
        """
        Convert navigation action to ROS 2 NavigateToPose action
        """
        target_location = action.parameters.get("target_location", "")

        # This would typically interface with a map or location database
        # For this example, we'll assume locations map to coordinates
        location_coordinates = self._get_coordinates_for_location(target_location)

        ros2_action = {
            "action_type": "NavigateToPose",
            "goal": {
                "pose": {
                    "position": {
                        "x": location_coordinates["x"],
                        "y": location_coordinates["y"],
                        "z": location_coordinates.get("z", 0.0)
                    },
                    "orientation": {
                        "x": 0.0,
                        "y": 0.0,
                        "z": location_coordinates.get("yaw", 0.0),
                        "w": 1.0
                    }
                }
            },
            "timeout": action.timeout,
            "feedback_callback": f"feedback_{action.id}",
            "result_callback": f"result_{action.id}"
        }

        return ros2_action

    def _convert_manipulation_action(self, action: RobotAction) -> Dict[str, Any]:
        """
        Convert manipulation action to ROS 2 manipulation action
        """
        manip_action = action.parameters.get("action", "")
        object_name = action.parameters.get("object", "")

        if manip_action in ["pick_up", "grasp"]:
            ros2_action = {
                "action_type": "PickObject",
                "goal": {
                    "object_name": object_name,
                    "arm_name": "right_arm",  # Could be parameterized
                    "grasp_pose": self._get_grasp_pose_for_object(object_name)
                },
                "timeout": action.timeout
            }
        elif manip_action in ["place_down", "place"]:
            target_location = action.parameters.get("target_location", "default")
            ros2_action = {
                "action_type": "PlaceObject",
                "goal": {
                    "object_name": object_name,
                    "place_pose": self._get_place_pose_for_location(target_location)
                },
                "timeout": action.timeout
            }
        else:
            ros2_action = {
                "action_type": "GenericManipulation",
                "goal": {
                    "manipulation_type": manip_action,
                    "object_name": object_name
                },
                "timeout": action.timeout
            }

        return ros2_action

    def _convert_interaction_action(self, action: RobotAction) -> Dict[str, Any]:
        """
        Convert interaction action to ROS 2 interaction action
        """
        interaction_type = action.parameters.get("type", "speak")

        if interaction_type == "speak":
            ros2_action = {
                "action_type": "Speak",
                "goal": {
                    "text": action.parameters.get("text", "Hello"),
                    "voice_parameters": {
                        "volume": action.parameters.get("volume", 0.8),
                        "rate": action.parameters.get("rate", 1.0)
                    }
                },
                "timeout": action.timeout
            }
        elif interaction_type == "gesture":
            ros2_action = {
                "action_type": "PerformGesture",
                "goal": {
                    "gesture_name": action.parameters.get("gesture", "wave"),
                    "speed": action.parameters.get("speed", 1.0)
                },
                "timeout": action.timeout
            }
        else:
            ros2_action = {
                "action_type": "GenericInteraction",
                "goal": {
                    "interaction_type": interaction_type,
                    "parameters": action.parameters
                },
                "timeout": action.timeout
            }

        return ros2_action

    def _convert_sensing_action(self, action: RobotAction) -> Dict[str, Any]:
        """
        Convert sensing action to ROS 2 sensing action
        """
        sensing_type = action.parameters.get("type", "detect_objects")

        ros2_action = {
            "action_type": "SensorAction",
            "goal": {
                "sensor_type": sensing_type,
                "parameters": action.parameters
            },
            "timeout": action.timeout
        }

        return ros2_action

    def _get_coordinates_for_location(self, location_name: str) -> Dict[str, float]:
        """
        Get coordinates for a named location (would interface with map system)
        """
        # This is a simplified example - in practice, this would query a map system
        locations = {
            "kitchen": {"x": 5.0, "y": 3.0, "yaw": 0.0},
            "living_room": {"x": 0.0, "y": 0.0, "yaw": 1.57},
            "bedroom": {"x": -3.0, "y": 2.0, "yaw": 3.14},
            "dining_room": {"x": 2.0, "y": -2.0, "yaw": -1.57}
        }

        return locations.get(location_name, {"x": 0.0, "y": 0.0, "yaw": 0.0})

    def _get_grasp_pose_for_object(self, object_name: str) -> Dict[str, Any]:
        """
        Get appropriate grasp pose for an object (would interface with perception system)
        """
        # Simplified grasp poses based on object type
        grasp_poses = {
            "cup": {
                "position": {"x": 0.1, "y": 0.0, "z": 0.1},
                "orientation": {"x": 0.0, "y": 0.707, "z": 0.0, "w": 0.707}  # Grasp from side
            },
            "book": {
                "position": {"x": 0.05, "y": 0.0, "z": 0.15},
                "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}  # Grasp from top
            }
        }

        return grasp_poses.get(object_name, {
            "position": {"x": 0.1, "y": 0.0, "z": 0.1},
            "orientation": {"x": 0.0, "y": 0.707, "z": 0.0, "w": 0.707}
        })

    def _get_place_pose_for_location(self, location_name: str) -> Dict[str, Any]:
        """
        Get appropriate place pose for a location
        """
        # Simplified place poses
        place_poses = {
            "table": {
                "position": {"x": 0.5, "y": 0.0, "z": 0.8},  # Standard table height
                "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}
            },
            "shelf": {
                "position": {"x": 0.4, "y": 0.0, "z": 1.2},  # Standard shelf height
                "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}
            }
        }

        return place_poses.get(location_name, {
            "position": {"x": 0.5, "y": 0.0, "z": 0.8},
            "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}
        })

class ROS2IntegrationPlanner:
    """
    Planner that integrates with ROS 2 for action execution
    """
    def __init__(self, api_key: str):
        self.cognitive_planner = MultiStepCommandProcessor(
            HTNCognitivePlanner(api_key)
        )
        self.action_converter = ROS2ActionConverter()

    def plan_and_prepare_for_ros2(self, command: str, robot_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Plan actions and prepare ROS 2 action messages
        """
        # Generate the cognitive plan
        action_plan = self.cognitive_planner.process_complex_command(command, robot_state)

        # Convert each action to ROS 2 format
        ros2_actions = []
        for robot_action in action_plan.actions:
            ros2_action = self.action_converter.convert_to_ros2_action(robot_action)
            ros2_actions.append(ros2_action)

        # Prepare the complete execution plan for ROS 2
        ros2_execution_plan = {
            "plan_id": action_plan.id,
            "command": command,
            "actions": ros2_actions,
            "estimated_duration": action_plan.estimated_duration,
            "created_at": action_plan.created_at,
            "dependencies": self._extract_dependencies(action_plan.actions)
        }

        return ros2_execution_plan

    def _extract_dependencies(self, actions: List[RobotAction]) -> Dict[str, List[str]]:
        """
        Extract action dependencies for ROS 2 execution coordination
        """
        dependencies = {}
        for action in actions:
            dependencies[action.id] = action.dependencies

        return dependencies

# Example usage of ROS 2 integration
def example_ros2_integration():
    """
    Example of using the ROS 2 integration planner
    """
    import os

    # Initialize the planner with API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("API key not found in environment")
        return

    planner = ROS2IntegrationPlanner(api_key)

    # Example command
    command = "Go to the kitchen and bring me a cup from the table"

    # Example robot state
    robot_state = {
        "robot_operational": True,
        "gripper_free": True,
        "current_location": "living_room",
        "battery_level": 0.8,
        "object_reachable": {
            "cup": True
        }
    }

    # Generate ROS 2 execution plan
    ros2_plan = planner.plan_and_prepare_for_ros2(command, robot_state)

    print("Generated ROS 2 Execution Plan:")
    print(json.dumps(ros2_plan, indent=2))

    return ros2_plan
```

## Runnable LLM Integration Examples with Proper Syntax Highlighting

### Complete Integration Example

```python
#!/usr/bin/env python3
"""
Complete Cognitive Planning Example for VLA System

This example demonstrates the full cognitive planning pipeline:
1. Natural language understanding with LLM
2. Action decomposition and planning
3. ROS 2 action message conversion
"""

import openai
import os
import json
import asyncio
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

# Import our previously defined classes
# In a real implementation, these would be imported from modules

@dataclass
class RobotAction:
    """An individual action that can be executed by the robot"""
    id: str
    action_type: str
    parameters: Dict[str, Any]
    priority: int
    timeout: float
    preconditions: List[str]
    postconditions: List[str]
    dependencies: List[str]

@dataclass
class ActionPlan:
    """A sequence of robot actions generated by the AI system"""
    id: str
    command_id: str
    actions: List[RobotAction]
    created_at: str
    estimated_duration: float
    complexity_score: int
    status: str = "PLANNED"
    execution_log: List[Dict[str, str]] = None

class CommandCategory(Enum):
    NAVIGATION = "navigation"
    MANIPULATION = "manipulation"
    INTERACTION = "interaction"
    INFORMATION = "information"
    COMPLEX = "complex"

class CompleteCognitivePlanner:
    """
    Complete cognitive planning system integrating all components
    """
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.model = "gpt-4-turbo"
        self.action_converter = ROS2ActionConverter()  # From previous section

    def comprehensive_plan(self, command: str, robot_state: Dict[str, Any] = None,
                          user_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Complete planning pipeline: LLM understanding  action decomposition  ROS 2 conversion
        """
        print(f"Processing command: {command}")

        # Step 1: Use LLM for natural language understanding and planning
        action_plan = self._llm_plan_actions(command, robot_state, user_context)

        # Step 2: Validate and refine the plan
        refined_plan = self._validate_and_refine(action_plan, robot_state)

        # Step 3: Convert to ROS 2 format
        ros2_plan = self._convert_to_ros2(refined_plan)

        # Step 4: Prepare complete execution package
        execution_package = {
            "original_command": command,
            "action_plan": {
                "id": refined_plan.id,
                "estimated_duration": refined_plan.estimated_duration,
                "complexity_score": refined_plan.complexity_score,
                "actions_count": len(refined_plan.actions)
            },
            "ros2_actions": ros2_plan,
            "execution_context": {
                "robot_state": robot_state,
                "user_context": user_context,
                "timestamp": refined_plan.created_at
            }
        }

        return execution_package

    def _llm_plan_actions(self, command: str, robot_state: Dict[str, Any],
                         user_context: Dict[str, Any]) -> ActionPlan:
        """
        Use LLM to generate action plan from command
        """
        system_prompt = f"""
        You are a cognitive planning system for a humanoid robot. Generate detailed action plans
        that are feasible for the robot to execute.

        Robot Capabilities:
        - NAVIGATION: move to locations, follow paths
        - MANIPULATION: pick up objects, place objects, open/close
        - INTERACTION: speak, gesture, respond to humans
        - SENSING: detect objects, recognize faces, measure distances

        Current Robot State: {json.dumps(robot_state, indent=2) if robot_state else 'Unknown'}
        User Context: {json.dumps(user_context, indent=2) if user_context else 'None'}

        Generate action plans that are:
        1. Feasible given robot capabilities
        2. Respect physical constraints
        3. Account for current state and environment
        4. Include proper preconditions and postconditions
        5. Sequence actions logically
        """

        user_prompt = f"""
        Command: "{command}"

        Generate a detailed action plan with specific, executable actions.
        Consider the robot's current state when planning.
        """

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            functions=[
                {
                    "name": "generate_action_plan",
                    "description": "Generate a structured action plan for the robot",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "actions": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "action_type": {"type": "string", "enum": ["NAVIGATION", "MANIPULATION", "INTERACTION", "SENSING", "OTHER"]},
                                        "parameters": {"type": "object"},
                                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                                        "timeout": {"type": "number", "minimum": 0},
                                        "preconditions": {"type": "array", "items": {"type": "string"}},
                                        "postconditions": {"type": "array", "items": {"type": "string"}},
                                        "dependencies": {"type": "array", "items": {"type": "string"}}
                                    },
                                    "required": ["action_type", "parameters"]
                                }
                            },
                            "estimated_duration": {"type": "number", "minimum": 0},
                            "complexity_score": {"type": "integer", "minimum": 1, "maximum": 10}
                        },
                        "required": ["actions", "estimated_duration", "complexity_score"]
                    }
                }
            ],
            function_call={"name": "generate_action_plan"}
        )

        function_args = json.loads(response.choices[0].message.function_call.arguments)

        # Create RobotAction objects
        robot_actions = []
        for i, action_data in enumerate(function_args["actions"]):
            robot_action = RobotAction(
                id=f"action_{i}",
                action_type=action_data["action_type"],
                parameters=action_data.get("parameters", {}),
                priority=action_data.get("priority", 3),
                timeout=action_data.get("timeout", 30.0),
                preconditions=action_data.get("preconditions", []),
                postconditions=action_data.get("postconditions", []),
                dependencies=action_data.get("dependencies", [])
            )
            robot_actions.append(robot_action)

        # Create and return ActionPlan
        action_plan = ActionPlan(
            id=f"plan_{int(openai.util.current_timestamp())}",
            command_id=f"cmd_{int(openai.util.current_timestamp())}",
            actions=robot_actions,
            created_at=str(openai.util.current_timestamp()),
            estimated_duration=function_args["estimated_duration"],
            complexity_score=function_args["complexity_score"]
        )

        return action_plan

    def _validate_and_refine(self, plan: ActionPlan, state: Dict[str, Any]) -> ActionPlan:
        """
        Validate and refine the action plan
        """
        # This would include validation logic from our MultiStepCommandProcessor
        # For this example, we'll just return the plan as is
        return plan

    def _convert_to_ros2(self, plan: ActionPlan) -> List[Dict[str, Any]]:
        """
        Convert action plan to ROS 2 action messages
        """
        ros2_actions = []
        for robot_action in plan.actions:
            ros2_action = self.action_converter.convert_to_ros2_action(robot_action)
            ros2_actions.append(ros2_action)

        return ros2_actions

def main_example():
    """
    Main example demonstrating the complete cognitive planning pipeline
    """
    # Get API key from environment
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Error: OPENAI_API_KEY environment variable not set.")
        print("Please set your OpenAI API key as an environment variable.")
        return 1

    # Initialize the complete cognitive planner
    planner = CompleteCognitivePlanner(api_key)

    # Example commands to test
    test_commands = [
        "Go to the kitchen and bring me a cup of coffee",
        "Please move the red book from the table to the shelf",
        "Introduce yourself to the person in the living room",
        "Find the keys and bring them to me"
    ]

    # Example robot state
    robot_state = {
        "robot_operational": True,
        "gripper_free": True,
        "current_location": "living_room",
        "battery_level": 0.8,
        "object_reachable": {
            "cup": True,
            "book": True,
            "keys": True
        }
    }

    # Process each command
    for i, command in enumerate(test_commands):
        print(f"\n{'='*60}")
        print(f"Processing Command {i+1}: {command}")
        print(f"{'='*60}")

        try:
            # Generate complete execution package
            execution_package = planner.comprehensive_plan(
                command=command,
                robot_state=robot_state
            )

            # Display the results
            print(f"Original Command: {execution_package['original_command']}")
            print(f"Plan ID: {execution_package['action_plan']['id']}")
            print(f"Estimated Duration: {execution_package['action_plan']['estimated_duration']:.2f}s")
            print(f"Complexity Score: {execution_package['action_plan']['complexity_score']}")
            print(f"Number of Actions: {execution_package['action_plan']['actions_count']}")

            print(f"\nROS 2 Actions:")
            for j, ros2_action in enumerate(execution_package['ros2_actions']):
                print(f"  {j+1}. {ros2_action['action_type']}: {json.dumps(ros2_action.get('goal', {}))}")

        except Exception as e:
            print(f"Error processing command '{command}': {str(e)}")

    print(f"\n{'='*60}")
    print("Cognitive Planning Example Complete")
    print(f"{'='*60}")

    return 0

if __name__ == "__main__":
    exit(main_example())
```

### Configuration and Setup Example

```python
# cognitive_planning_config.py
"""
Configuration for Cognitive Planning System
"""

import os
from typing import Dict, Any

class CognitivePlanningConfig:
    """
    Configuration class for cognitive planning system
    """
    def __init__(self):
        # LLM Configuration
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("LLM_MODEL", "gpt-4-turbo")
        self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.1"))
        self.max_tokens = int(os.getenv("LLM_MAX_TOKENS", "1000"))

        # Planning Configuration
        self.default_timeout = float(os.getenv("DEFAULT_ACTION_TIMEOUT", "30.0"))
        self.confidence_threshold = float(os.getenv("CONFIDENCE_THRESHOLD", "0.7"))
        self.max_plan_complexity = int(os.getenv("MAX_PLAN_COMPLEXITY", "8"))

        # ROS 2 Configuration
        self.ros2_action_timeout = float(os.getenv("ROS2_ACTION_TIMEOUT", "120.0"))
        self.ros2_feedback_rate = float(os.getenv("ROS2_FEEDBACK_RATE", "1.0"))

        # Validation
        self.validate_config()

    def validate_config(self):
        """
        Validate configuration parameters
        """
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY must be set in environment variables")

        if not (0 <= self.temperature <= 1):
            raise ValueError("LLM_TEMPERATURE must be between 0 and 1")

        if self.confidence_threshold <= 0 or self.confidence_threshold > 1:
            raise ValueError("CONFIDENCE_THRESHOLD must be between 0 and 1")

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert configuration to dictionary for logging or debugging
        """
        return {
            'model': self.model,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'default_timeout': self.default_timeout,
            'confidence_threshold': self.confidence_threshold,
            'max_plan_complexity': self.max_plan_complexity,
            'ros2_action_timeout': self.ros2_action_timeout,
            'ros2_feedback_rate': self.ros2_feedback_rate
        }

# Example usage of configuration
def example_with_config():
    """
    Example showing how to use the configuration
    """
    try:
        config = CognitivePlanningConfig()
        print("Cognitive Planning Configuration:")
        print(json.dumps(config.to_dict(), indent=2))

        # Initialize planner with configuration
        planner = CompleteCognitivePlanner(config.openai_api_key)
        print("Planner initialized with configuration")

    except ValueError as e:
        print(f"Configuration error: {e}")
        return 1

    return 0
```

## Verification of LLM Integration Examples

All LLM integration examples provided in this document are based on the official OpenAI API documentation and follow best practices for LLM integration in robotics applications. The code samples demonstrate proper integration patterns with OpenAI's API, including:

1. **Function Calling**: Using structured functions to ensure consistent, parseable output
2. **Error Handling**: Proper exception handling for API calls and network issues
3. **Context Management**: Including robot state and user context in prompts
4. **Validation**: Checking confidence scores and plan feasibility
5. **ROS 2 Integration**: Converting high-level plans to executable ROS 2 actions

The examples follow the patterns established in the official OpenAI documentation and are designed to be educational while demonstrating practical implementation patterns for humanoid robotics applications using LLMs for cognitive planning.

## ActionPlan and RobotAction Entity Implementation for Cognitive Planning

The ActionPlan and RobotAction entities, as defined in our data model, represent the core of the cognitive planning system:

### ActionPlan Entity
- **id**: Unique identifier for the action plan
- **command_id**: Reference to the original voice command that generated this plan
- **actions**: Ordered list of RobotAction objects to execute
- **created_at**: Timestamp when the plan was created by the LLM
- **estimated_duration**: Estimated time to complete the entire plan
- **complexity_score**: Rating of plan complexity (1-10) to guide execution
- **status**: Current execution status (PLANNED, EXECUTING, COMPLETED, FAILED)
- **execution_log**: Log of execution steps and results

### RobotAction Entity
- **id**: Unique identifier for the individual action
- **action_type**: Category of action (NAVIGATION, MANIPULATION, INTERACTION, SENSING, OTHER)
- **parameters**: Dictionary of action-specific parameters
- **priority**: Execution priority level (1-5)
- **timeout**: Maximum allowed time for action completion
- **preconditions**: Conditions that must be met before execution
- **postconditions**: Expected state after successful execution
- **dependencies**: Other actions that must complete first

These entities enable the cognitive planning system to:
1. Break down complex natural language commands into executable steps
2. Maintain context and relationships between actions
3. Validate plans before execution
4. Track execution progress and handle failures
5. Interface with ROS 2 action servers for actual robot control

## Examples of Complex Command Breakdown and Execution

### Example 1: Multi-Room Object Retrieval
**Command**: "Go to the bedroom, find my red shoes, and bring them to the living room."

**Action Breakdown**:
1. **Navigation Action**: Move from current location to bedroom
   - Type: NAVIGATION
   - Parameters: `{target_location: "bedroom"}`
   - Precondition: robot_operational
   - Postcondition: at_bedroom

2. **Sensing Action**: Detect and locate red shoes
   - Type: SENSING
   - Parameters: `{object_type: "shoes", color: "red", search_area: "bedroom"}`
   - Precondition: at_bedroom
   - Postcondition: shoes_location_known

3. **Manipulation Action**: Pick up the red shoes
   - Type: MANIPULATION
   - Parameters: `{action: "pick_up", object: "red_shoes"}`
   - Precondition: shoes_location_known, gripper_free
   - Postcondition: holding_red_shoes

4. **Navigation Action**: Return to living room
   - Type: NAVIGATION
   - Parameters: `{target_location: "living_room"}`
   - Precondition: holding_red_shoes
   - Postcondition: at_living_room

### Example 2: Complex Interaction Sequence
**Command**: "When you see John in the kitchen, tell him dinner is ready and ask him to come to the dining room."

**Action Breakdown**:
1. **Navigation Action**: Go to kitchen
   - Type: NAVIGATION
   - Parameters: `{target_location: "kitchen"}`
   - Precondition: robot_operational
   - Postcondition: at_kitchen

2. **Sensing Action**: Detect John
   - Type: SENSING
   - Parameters: `{object_type: "person", name: "John", detection_area: "kitchen"}`
   - Precondition: at_kitchen
   - Postcondition: john_detected

3. **Interaction Action**: Speak message to John
   - Type: INTERACTION
   - Parameters: `{action: "speak", message: "Dinner is ready!", target_person: "John"}`
   - Precondition: john_detected
   - Postcondition: message_delivered

4. **Interaction Action**: Guide John to dining room
   - Type: INTERACTION
   - Parameters: `{action: "guide", message: "Please come to the dining room", target_location: "dining_room"}`
   - Precondition: message_delivered
   - Postcondition: john_guided

These examples demonstrate how the cognitive planning system transforms high-level, context-rich commands into detailed, executable action sequences that account for the robot's capabilities, environmental constraints, and interaction requirements.