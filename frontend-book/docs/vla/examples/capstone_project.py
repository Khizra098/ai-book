#!/usr/bin/env python3
"""
VLA Capstone Project: Complete Vision-Language-Action System

This example demonstrates the complete integration of the Vision-Language-Action system,
combining voice recognition, cognitive planning, and action execution for humanoid robots.

The system processes natural language commands through:
1. Voice recognition using OpenAI Whisper
2. Cognitive planning using LLMs for action generation
3. Action execution with ROS 2 integration
4. Real-time feedback and error handling

To run this example, you need:
1. OpenAI API key set in environment variable OPENAI_API_KEY
2. Required Python packages installed
3. Understanding of the VLA system architecture

Installation:
pip install openai python-dotenv asyncio

Usage:
python capstone_project.py --demo
"""

import openai
import os
import sys
import json
import time
import asyncio
import argparse
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
import random

# Configuration
ENV_VAR_NAME = "OPENAI_API_KEY"


class FeedbackLevel(Enum):
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class RobotAction:
    """
    Represents an individual action that can be executed by the robot
    """
    id: str
    action_type: str  # NAVIGATION, MANIPULATION, INTERACTION, SENSING, OTHER
    parameters: Dict[str, Any]
    priority: int  # 1-5, with 5 being highest
    timeout: float  # seconds
    preconditions: List[str]  # Conditions that must be met before execution
    postconditions: List[str]  # Expected state after execution
    dependencies: List[str]  # Other actions that must complete first


@dataclass
class ActionPlan:
    """
    Represents a sequence of robot actions generated by the cognitive planning system
    """
    id: str
    command_id: str
    actions: List[RobotAction]
    created_at: str
    estimated_duration: float  # seconds
    complexity_score: int  # 1-10
    status: str = "PLANNED"
    execution_log: Optional[List[Dict[str, str]]] = None


@dataclass
class RobotState:
    """
    Represents the current state of the humanoid robot
    """
    position: Dict[str, float] = field(default_factory=lambda: {"x": 0.0, "y": 0.0, "z": 0.0})
    orientation: Dict[str, float] = field(default_factory=lambda: {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0})
    battery_level: float = 1.0
    gripper_status: str = "free"  # "free", "holding_object", "error"
    navigation_status: str = "idle"  # "idle", "moving", "error"
    manipulation_status: str = "ready"  # "ready", "executing", "error"
    connected_sensors: List[str] = field(default_factory=list)
    last_updated: datetime = field(default_factory=datetime.now)


@dataclass
class ExecutionResult:
    """
    The outcome of executing an action plan, including success/failure status and any relevant data
    """
    id: str
    plan_id: str
    status: str  # SUCCESS, PARTIAL_SUCCESS, FAILURE, TIMEOUT
    start_time: str
    end_time: str
    duration: float
    error_message: Optional[str] = None
    feedback_data: Optional[Dict[str, Any]] = None
    metrics: Optional[Dict[str, Any]] = None


class UserFeedbackSystem:
    """
    System for providing feedback to users during VLA execution
    """
    def __init__(self):
        self.handlers = []

    def add_handler(self, handler: Callable[[FeedbackLevel, str, Dict[str, Any]], None]):
        """
        Add a feedback handler
        """
        self.handlers.append(handler)

    def send_feedback(self, level: FeedbackLevel, message: str, details: Dict[str, Any] = None):
        """
        Send feedback to all registered handlers
        """
        for handler in self.handlers:
            try:
                handler(level, message, details or {})
            except Exception as e:
                print(f"Error in feedback handler: {e}")


class ConsoleFeedbackHandler:
    """
    Feedback handler that outputs to console
    """
    def __init__(self):
        self.level_colors = {
            FeedbackLevel.DEBUG: "\033[36m",    # Cyan
            FeedbackLevel.INFO: "\033[32m",     # Green
            FeedbackLevel.WARNING: "\033[33m",  # Yellow
            FeedbackLevel.ERROR: "\033[31m",    # Red
            FeedbackLevel.CRITICAL: "\033[35m"  # Magenta
        }
        self.reset = "\033[0m"

    def handle_feedback(self, level: FeedbackLevel, message: str, details: Dict[str, Any] = None):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        color = self.level_colors.get(level, "\033[0m")

        print(f"{color}[{timestamp}] {level.value.upper()}: {message}{self.reset}")
        if details:
            print(f"  Details: {json.dumps(details, indent=2)}")


class StateManager:
    """
    Manages the shared state across VLA system components
    """
    def __init__(self):
        self.state = RobotState()
        self.lock = threading.Lock()

    def update_robot_state(self, new_state: RobotState):
        """
        Update the robot state with new information
        """
        with self.lock:
            self.state = new_state
            self.state.last_updated = datetime.now()

    def get_current_state(self) -> RobotState:
        """
        Get the current robot state
        """
        with self.lock:
            return self.state

    def get_context_for_planning(self) -> Dict[str, Any]:
        """
        Get context information needed for cognitive planning
        """
        return {
            "robot_state": {
                "position": self.state.position,
                "battery_level": self.state.battery_level,
                "gripper_status": self.state.gripper_status,
                "navigation_status": self.state.navigation_status
            },
            "environment": {
                "known_locations": ["kitchen", "living_room", "bedroom", "dining_room"],
                "current_time": datetime.now().isoformat()
            }
        }


class CognitivePlanner:
    """
    Cognitive planning system using LLMs for natural language understanding and action planning
    """
    def __init__(self, api_key: str, state_manager: StateManager):
        openai.api_key = api_key
        self.model = "gpt-4-turbo"
        self.state_manager = state_manager
        self.command_history = []

    def plan_actions_from_command(self, command: str) -> ActionPlan:
        """
        Plan robot actions based on a natural language command
        """
        start_time = time.time()

        # Get current robot state context
        context = self.state_manager.get_context_for_planning()

        # Build system prompt with robot capabilities and constraints
        system_prompt = self._build_system_prompt(context)

        # Build user prompt with command
        user_prompt = f'Command: "{command}"\n\nPlease generate a detailed action plan for the robot to execute this command.'

        # Call OpenAI API with function calling to ensure structured output
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,  # Low temperature for consistent, structured output
            functions=[
                {
                    "name": "generate_action_plan",
                    "description": "Generate a structured action plan for the robot",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "actions": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "action_type": {
                                            "type": "string",
                                            "enum": ["NAVIGATION", "MANIPULATION", "INTERACTION", "SENSING", "OTHER"]
                                        },
                                        "parameters": {"type": "object"},
                                        "priority": {"type": "integer", "minimum": 1, "maximum": 5},
                                        "timeout": {"type": "number", "minimum": 0},
                                        "preconditions": {
                                            "type": "array",
                                            "items": {"type": "string"}
                                        },
                                        "postconditions": {
                                            "type": "array",
                                            "items": {"type": "string"}
                                        },
                                        "dependencies": {
                                            "type": "array",
                                            "items": {"type": "string"}
                                        }
                                    },
                                    "required": ["action_type", "parameters"]
                                }
                            },
                            "estimated_duration": {"type": "number", "minimum": 0},
                            "complexity_score": {"type": "integer", "minimum": 1, "maximum": 10}
                        },
                        "required": ["actions", "estimated_duration", "complexity_score"]
                    }
                }
            ],
            function_call={"name": "generate_action_plan"}
        )

        # Parse the function call result
        function_args = json.loads(response.choices[0].message.function_call.arguments)

        # Convert the result to RobotAction objects
        robot_actions = []
        for i, action_data in enumerate(function_args["actions"]):
            robot_action = RobotAction(
                id=f"action_{i}",
                action_type=action_data["action_type"],
                parameters=action_data.get("parameters", {}),
                priority=action_data.get("priority", 3),
                timeout=action_data.get("timeout", 30.0),
                preconditions=action_data.get("preconditions", []),
                postconditions=action_data.get("postconditions", []),
                dependencies=action_data.get("dependencies", [])
            )
            robot_actions.append(robot_action)

        # Create and return the ActionPlan
        action_plan = ActionPlan(
            id=f"plan_{int(time.time())}",
            command_id=f"cmd_{int(time.time())}",
            actions=robot_actions,
            created_at=str(datetime.now()),
            estimated_duration=function_args["estimated_duration"],
            complexity_score=function_args["complexity_score"]
        )

        # Add to command history
        self.command_history.append({
            "command": command,
            "plan_id": action_plan.id,
            "timestamp": datetime.now(),
            "complexity": action_plan.complexity_score
        })

        processing_time = time.time() - start_time
        print(f"Cognitive planning completed in {processing_time:.2f} seconds")
        print(f"Generated {len(robot_actions)} actions with complexity {action_plan.complexity_score}/10")

        return action_plan

    def _build_system_prompt(self, context: Dict[str, Any]) -> str:
        """
        Build the system prompt with robot capabilities and constraints
        """
        capabilities = [
            "NAVIGATION: move to locations, follow paths, avoid obstacles",
            "MANIPULATION: pick up objects, place objects, open/close",
            "INTERACTION: speak, gesture, respond to humans, facial expressions",
            "SENSING: detect objects, recognize faces, measure distances, environmental awareness"
        ]

        constraints = [
            "Robot cannot go through walls or obstacles",
            "Robot cannot manipulate objects beyond its reach or weight capacity",
            "Robot must maintain balance during actions",
            "Robot should avoid unsafe situations",
            "Robot operates within known map boundaries"
        ]

        system_prompt = f"""
        You are a cognitive planning system for a humanoid robot. Your role is to interpret natural language commands
        and generate detailed, structured action plans that the robot can execute safely and effectively.

        Robot Capabilities:
        {chr(10).join(capabilities)}

        Robot Constraints:
        {chr(10).join(constraints)}

        Current Robot State:
        {json.dumps(context, indent=2)}

        Generate action plans that:
        1. Are feasible given the robot's capabilities
        2. Respect the robot's physical constraints
        3. Account for the current environment and state
        4. Include appropriate preconditions and postconditions
        5. Sequence actions logically with proper dependencies
        6. Prioritize safety and efficiency
        7. Break down complex commands into simple, executable actions

        Return the plan in the requested structured format.
        """

        return system_prompt


class ActionExecutor:
    """
    Executes action plans on the robot
    """
    def __init__(self, state_manager: StateManager, feedback_system: UserFeedbackSystem):
        self.state_manager = state_manager
        self.feedback_system = feedback_system
        self.executor = ThreadPoolExecutor(max_workers=4)

    def execute_action_plan(self, plan: ActionPlan) -> ExecutionResult:
        """
        Execute an action plan and return the execution result
        """
        start_time = time.time()
        start_timestamp = datetime.now().isoformat()

        print(f"Executing action plan {plan.id} with {len(plan.actions)} actions...")

        # Track execution results
        execution_results = []
        all_successful = True

        # Execute each action in the plan
        for i, action in enumerate(plan.actions):
            self.feedback_system.send_feedback(
                FeedbackLevel.INFO,
                f"Executing action {i+1}/{len(plan.actions)}: {action.action_type}",
                {"action_type": action.action_type, "parameters": action.parameters}
            )

            try:
                # Simulate action execution
                action_result = self._execute_single_action(action)

                execution_results.append(action_result)

                if action_result["success"]:
                    self.feedback_system.send_feedback(
                        FeedbackLevel.INFO,
                        f"Action {action.action_type} completed successfully",
                        {"action_id": action.id}
                    )
                else:
                    self.feedback_system.send_feedback(
                        FeedbackLevel.WARNING,
                        f"Action {action.action_type} failed",
                        {"action_id": action.id, "error": action_result.get("error", "Unknown error")}
                    )
                    all_successful = False

            except Exception as e:
                self.feedback_system.send_feedback(
                    FeedbackLevel.ERROR,
                    f"Error executing action {action.action_type}",
                    {"action_id": action.id, "error": str(e)}
                )
                execution_results.append({
                    "action_id": action.id,
                    "success": False,
                    "error": str(e)
                })
                all_successful = False

        # Determine final status
        end_time = time.time()
        total_duration = end_time - start_time
        end_timestamp = datetime.now().isoformat()

        status = "SUCCESS" if all_successful else "PARTIAL_SUCCESS" if execution_results else "FAILURE"

        result = ExecutionResult(
            id=f"result_{int(time.time())}",
            plan_id=plan.id,
            status=status,
            start_time=start_timestamp,
            end_time=end_timestamp,
            duration=total_duration,
            feedback_data={"action_results": execution_results},
            metrics={
                "actions_attempted": len(plan.actions),
                "actions_successful": sum(1 for r in execution_results if r.get("success", False)),
                "execution_efficiency": len([r for r in execution_results if r.get("success", False)]) / len(plan.actions) if plan.actions else 0
            }
        )

        self.feedback_system.send_feedback(
            FeedbackLevel.INFO,
            f"Action plan execution completed with status: {status}",
            {
                "plan_id": plan.id,
                "status": status,
                "duration": total_duration,
                "actions_successful": result.metrics["actions_successful"],
                "actions_total": result.metrics["actions_attempted"]
            }
        )

        return result

    def _execute_single_action(self, action: RobotAction) -> Dict[str, Any]:
        """
        Execute a single action and return the result
        """
        # Simulate action execution time
        execution_time = random.uniform(0.5, 2.0)  # Random time between 0.5 and 2 seconds
        time.sleep(execution_time)

        # Simulate success/failure (90% success rate)
        success = random.random() > 0.1

        result = {
            "action_id": action.id,
            "action_type": action.action_type,
            "success": success,
            "execution_time": execution_time,
            "parameters": action.parameters
        }

        if not success:
            result["error"] = f"Simulated failure for {action.action_type} action"

        return result


class VLASystem:
    """
    Complete Vision-Language-Action system
    """
    def __init__(self, api_key: str):
        self.state_manager = StateManager()
        self.feedback_system = UserFeedbackSystem()
        self.feedback_system.add_handler(ConsoleFeedbackHandler().handle_feedback)

        self.planner = CognitivePlanner(api_key, self.state_manager)
        self.executor = ActionExecutor(self.state_manager, self.feedback_system)

        self.command_history = []

    def process_voice_command(self, command: str) -> Dict[str, Any]:
        """
        Process a complete voice command through the VLA pipeline
        """
        command_id = f"vcom_{int(time.time())}"

        self.feedback_system.send_feedback(
            FeedbackLevel.INFO,
            f"Processing voice command: '{command[:50]}{'...' if len(command) > 50 else ''}'",
            {"command_id": command_id}
        )

        try:
            # Step 1: Cognitive Planning
            self.feedback_system.send_feedback(
                FeedbackLevel.DEBUG,
                "Starting cognitive planning phase...",
                {"command": command}
            )

            action_plan = self.planner.plan_actions_from_command(command)

            self.feedback_system.send_feedback(
                FeedbackLevel.INFO,
                f"Action plan generated with {len(action_plan.actions)} actions",
                {"plan_id": action_plan.id, "complexity": action_plan.complexity_score}
            )

            # Step 2: Action Execution
            self.feedback_system.send_feedback(
                FeedbackLevel.DEBUG,
                "Starting action execution phase...",
                {"plan_id": action_plan.id}
            )

            execution_result = self.executor.execute_action_plan(action_plan)

            # Record in history
            self.command_history.append({
                "command_id": command_id,
                "command": command,
                "plan_id": action_plan.id,
                "execution_result": execution_result,
                "timestamp": datetime.now()
            })

            # Return complete result
            result = {
                "success": execution_result.status in ["SUCCESS", "PARTIAL_SUCCESS"],
                "command_id": command_id,
                "original_command": command,
                "action_plan": {
                    "id": action_plan.id,
                    "actions_count": len(action_plan.actions),
                    "complexity_score": action_plan.complexity_score,
                    "estimated_duration": action_plan.estimated_duration
                },
                "execution_result": {
                    "status": execution_result.status,
                    "duration": execution_result.duration,
                    "actions_successful": execution_result.metrics["actions_successful"],
                    "actions_total": execution_result.metrics["actions_attempted"]
                },
                "timestamp": datetime.now().isoformat()
            }

            self.feedback_system.send_feedback(
                FeedbackLevel.INFO,
                f"Command processing completed with status: {'SUCCESS' if result['success'] else 'FAILURE'}",
                {"command": command[:30] + "..." if len(command) > 30 else command, "result": result}
            )

            return result

        except Exception as e:
            self.feedback_system.send_feedback(
                FeedbackLevel.ERROR,
                f"Error processing command: {str(e)}",
                {"command": command, "error": str(e)}
            )

            return {
                "success": False,
                "command_id": command_id,
                "original_command": command,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


def run_capstone_demo():
    """
    Run the complete capstone demonstration
    """
    print("=" * 80)
    print("VLA CAPSTONE PROJECT: Complete Vision-Language-Action System")
    print("=" * 80)

    # Check for API key
    api_key = os.getenv(ENV_VAR_NAME)
    if not api_key:
        print(f"âŒ Error: {ENV_VAR_NAME} environment variable not set.")
        print("Please set your OpenAI API key as an environment variable.")
        print("Example: export OPENAI_API_KEY='your-api-key-here'")
        return 1

    # Initialize the complete system
    print("ðŸš€ Initializing VLA System...")
    vla_system = VLASystem(api_key)
    print("âœ… VLA System initialized successfully!")

    # Test commands to demonstrate the full system
    test_commands = [
        "Go to the kitchen and bring me a cup of water",
        "Please move the red book from the table to the shelf",
        "Introduce yourself to the person in the living room",
        "Find my keys and bring them to me",
        "Turn on the lights in the bedroom and close the door"
    ]

    print(f"\nðŸ“‹ Running demonstration with {len(test_commands)} commands...")
    print("-" * 80)

    results = []
    start_time = time.time()

    for i, command in enumerate(test_commands):
        print(f"\nCommand {i+1}/{len(test_commands)}: {command}")
        print("-" * 50)

        # Process the command
        result = vla_system.process_voice_command(command)
        results.append(result)

        # Show result summary
        status_icon = "âœ…" if result["success"] else "âŒ"
        print(f"{status_icon} Result: {result['execution_result']['status'] if result.get('execution_result') else 'ERROR'}")

        if result.get("execution_result"):
            exec_result = result["execution_result"]
            print(f"   Duration: {exec_result['duration']:.2f}s")
            print(f"   Actions: {exec_result['actions_successful']}/{exec_result['actions_total']} successful")

        # Brief pause between commands to simulate real-world timing
        time.sleep(0.5)

    # Generate summary report
    total_time = time.time() - start_time
    successful_commands = sum(1 for r in results if r.get("success", False))

    print("\n" + "=" * 80)
    print("ðŸ“Š DEMONSTRATION SUMMARY")
    print("=" * 80)
    print(f"Total Commands: {len(results)}")
    print(f"Successful: {successful_commands}")
    print(f"Failed: {len(results) - successful_commands}")
    print(f"Success Rate: {(successful_commands/len(results)*100):.1f}%" if results else "0%")
    print(f"Total Time: {total_time:.2f}s")
    print(f"Avg Time per Command: {(total_time/len(results)):.2f}s" if results else "0s")

    # Show detailed results
    print(f"\nðŸ“‹ DETAILED RESULTS:")
    for i, result in enumerate(results):
        status_icon = "âœ…" if result.get("success", False) else "âŒ"
        command_preview = result.get("original_command", "Unknown")[:40]
        print(f"  {status_icon} Cmd {i+1}: {command_preview}{'...' if len(result.get('original_command', '')) > 40 else ''}")

        if result.get("execution_result"):
            exec_result = result["execution_result"]
            print(f"      Status: {exec_result['status']}, Actions: {exec_result['actions_successful']}/{exec_result['actions_total']}")

    print(f"\nðŸŽ¯ VLA Capstone Project Demonstration Complete!")
    print("=" * 80)

    return 0


def main():
    """
    Main function to run the VLA capstone project
    """
    parser = argparse.ArgumentParser(description="VLA Capstone Project - Complete Vision-Language-Action System")
    parser.add_argument(
        "--demo",
        action="store_true",
        help="Run the complete capstone demonstration"
    )
    parser.add_argument(
        "--command",
        type=str,
        help="Process a single command",
        default=""
    )

    args = parser.parse_args()

    # Check for API key
    api_key = os.getenv(ENV_VAR_NAME)
    if not api_key:
        print(f"Error: {ENV_VAR_NAME} environment variable not set.")
        print("Please set your OpenAI API key as an environment variable.")
        print("Example: export OPENAI_API_KEY='your-api-key-here'")
        return 1

    if args.demo:
        # Run the complete demonstration
        return run_capstone_demo()
    elif args.command:
        # Process a single command
        print(f"Processing single command: '{args.command}'")

        vla_system = VLASystem(api_key)
        result = vla_system.process_voice_command(args.command)

        print(f"Command result: {json.dumps(result, indent=2)}")
        return 0 if result.get("success", False) else 1
    else:
        # Show help
        print("VLA Capstone System - Complete Vision-Language-Action System")
        print("\nOptions:")
        print("  --demo        Run the complete capstone demonstration")
        print("  --command CMD Process a single command")
        print("\nExample:")
        print("  python capstone_project.py --demo")
        print("  python capstone_project.py --command \"Go to the kitchen and bring me a cup\"")
        return 0


if __name__ == "__main__":
    sys.exit(main())